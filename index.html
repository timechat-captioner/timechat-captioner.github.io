<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions">
  <meta property="og:title" content="TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions"/>
  <meta property="og:description" content="TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions"/>
  <meta property="og:url" content="https://timechat-captioner.github.io/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="/static/image/teaser.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions">
  <meta name="twitter:description" content="TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/image/teaser.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Dense Video Captioning, Audio-Visual Understanding, Multimodal Learning, Video Description, Scene Segmentation">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">TimeChat-Captioner: Scripting Multi-Scene Videos<br>with Time-Aware and Structural<br>Audio-Visual Captions</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://yaolinli.github.io/" target="_blank">Linli Yao</a><sup>1,2</sup>,</span>
                <span class="author-block">
                  <a href="https://github.com/wyclike/" target="_blank">Yuancheng Wei</a><sup>3</sup>,</span>
                  <span class="author-block">
                    Yaojie Zhang<sup>4</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://lilei-nlp.github.io/" target="_blank">Lei Li</a><sup>5</sup>,
                  </span>
                  <span class="author-block">
                    Xinlong Chen<sup>6,2</sup>,
                  </span>
                  <span class="author-block">
                    Feifan Song<sup>1</sup>,
                  </span>
                  <span class="author-block">
                    Ziyue Wang<sup>1</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://ouyangkun10.github.io/" target="_blank">Kun Ouyang</a><sup>1</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://llyx97.github.io/" target="_blank">Yuanxin Liu</a><sup>1</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://ikekonglp.github.io/" target="_blank">Lingpeng Kong</a><sup>5</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://leuchine.github.io/" target="_blank">Qi Liu</a><sup>5</sup>,
                  </span>
                  <span class="author-block">
                    Pengfei Wan<sup>2</sup>,
                  </span>
                  <span class="author-block">
                    Kun Gai<sup>2</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=COdftTMAAAAJ&hl=en&oi=ao" target="_blank">Yuanxing Zhang</a><sup>2</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://xusun26.github.io/" target="_blank">Xu Sun</a><sup>1</sup>
                  </span>
                </div>

                  <div class="is-size-5 publication-authors">
                    <span class="affiliation-block"><sup>1</sup>Peking University, <sup>2</sup>Kling Team, Kuaishou Technology, <sup>3</sup>South China University of Technology,<br> <sup>4</sup>University of Electronic Science and Technology of China, <sup>5</sup>The University of Hong Kong, <sup>6</sup>Institute of Automation, Chinese Academy of Sciences</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">

                      <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2602.08711" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Github link -->
                    <span class="link-block">
                      <a href="https://github.com/yaolinli/TimeChat-Captioner" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code & Data</span>
                      </a>
                    </span>

                    <!-- Benchmark link -->
                    <span class="link-block">
                      <a href="https://huggingface.co/datasets/yaolily/OmniDCBench" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-save"></i>
                      </span>
                      <span>Benchmark</span>
                      </a>
                    </span>

                    <!-- Dataset link -->
                    <span class="link-block">
                      <a href="42K" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-database"></i>
                      </span>
                      <span>Dataset</span>
                      </a>
                    </span>

                    <!-- Checkpoints link -->
                    <span class="link-block">
                      <a href="https://huggingface.co/yaolily/TimeChat-Captioner-GRPO-7B" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-save"></i>
                      </span>
                      <span>Model</span>
                      </a>
                    </span>

                  </div>
                </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- New Case Study Section -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container" style="max-width: 1000px; margin: 0 auto;">
        <div style="margin-bottom: 3rem;"></div>
      </div>
    </div>
  </div>
</section>

<!-- Interactive Demo Section -->
<section class="section" id="demo" style="margin-top: 1rem; margin-bottom: 2rem; background-color: #fafbfc; padding: 3rem 0;">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered" style="margin-bottom: 2rem;">Interactive Demo</h2>
    
    <div style="margin-bottom: 2rem;">
      <video controls id="demoVideo" style="width: 100%; max-width: 900px; display: block; margin: 0 auto; border-radius: 8px; box-shadow: 0 8px 32px rgba(0, 0, 0, 0.12); border: 1px solid #e5e7eb; aspect-ratio: auto; object-fit: contain;">
        <source src="static/videos/case_cn.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
    </div>
    
    <!-- Enhanced Tip Bar -->
    <div style="background: #f5f5f5; color: #374151; padding: 1rem 1.5rem; border-radius: 8px; margin-bottom: 2rem; display: flex; align-items: center; justify-content: center; border-left: 4px solid hsl(204, 86%, 53%);">
      <span style="font-size: 1.2rem; margin-right: 0.5rem;">üí°</span>
      <span style="font-weight: 500;">Tip: Select a segment below to view detailed annotations</span>
    </div>
    
    <!-- Scrollable Timeline with Hidden Scrollbar -->
    <div style="overflow-x: auto; white-space: nowrap; padding: 1.5rem; background-color: #ffffff; border-radius: 12px; margin-bottom: 2rem; box-shadow: 0 2px 8px rgba(0,0,0,0.05); position: relative;">
      <style>
        .timeline-container::-webkit-scrollbar { display: none; }
        .timeline-container { -ms-overflow-style: none; scrollbar-width: none; }
        .demo-segment:hover {
          transform: translateY(-2px);
          box-shadow: 0 4px 12px rgba(0,0,0,0.15);
        }
        .demo-segment.active {
          background: #f0f7ff !important;
          color: hsl(204, 86%, 53%) !important;
          border-bottom-color: hsl(204, 86%, 53%) !important;
        }
        .demo-segment.active .segment-indicator {
          background: hsl(204, 86%, 53%) !important;
        }
      </style>
      <div class="timeline-container" style="display: flex; gap: 1rem;">
        <div class="demo-segment" data-segment="0" style="flex: 0 0 280px; background: #f9fafb; padding: 1.5rem; border-radius: 8px; cursor: pointer; border-bottom: 3px solid #e5e7eb; transition: all 0.3s ease; position: relative;">
          <h5 class="title is-6" style="margin-bottom: 0.5rem; color: #374151;">00:00-00:10</h5>
          <p style="font-size: 0.85rem; color: #6b7280; margin: 0;">Detailed Events</p>
          <div class="segment-indicator" style="position: absolute; bottom: 0; left: 0; right: 0; height: 3px; background: #95A5B0; border-radius: 0 0 8px 8px;"></div>
        </div>
        <div class="demo-segment" data-segment="1" style="flex: 0 0 280px; background: #f9fafb; padding: 1.5rem; border-radius: 8px; cursor: pointer; border-bottom: 3px solid #e5e7eb; transition: all 0.3s ease; position: relative;">
          <h5 class="title is-6" style="margin-bottom: 0.5rem; color: #374151;">00:11-00:20</h5>
          <p style="font-size: 0.85rem; color: #6b7280; margin: 0;">Detailed Events</p>
          <div class="segment-indicator" style="position: absolute; bottom: 0; left: 0; right: 0; height: 3px; background: #95A5B0; border-radius: 0 0 8px 8px;"></div>
        </div>
        <div class="demo-segment" data-segment="2" style="flex: 0 0 280px; background: #f9fafb; padding: 1.5rem; border-radius: 8px; cursor: pointer; border-bottom: 3px solid #e5e7eb; transition: all 0.3s ease; position: relative;">
          <h5 class="title is-6" style="margin-bottom: 0.5rem; color: #374151;">00:21-00:28</h5>
          <p style="font-size: 0.85rem; color: #6b7280; margin: 0;">Detailed Events</p>
          <div class="segment-indicator" style="position: absolute; bottom: 0; left: 0; right: 0; height: 3px; background: #95A5B0; border-radius: 0 0 8px 8px;"></div>
        </div>
        <div class="demo-segment" data-segment="3" style="flex: 0 0 280px; background: #f9fafb; padding: 1.5rem; border-radius: 8px; cursor: pointer; border-bottom: 3px solid #e5e7eb; transition: all 0.3s ease; position: relative;">
          <h5 class="title is-6" style="margin-bottom: 0.5rem; color: #374151;">00:29-00:36</h5>
          <p style="font-size: 0.85rem; color: #6b7280; margin: 0;">Detailed Events</p>
          <div class="segment-indicator" style="position: absolute; bottom: 0; left: 0; right: 0; height: 3px; background: #95A5B0; border-radius: 0 0 8px 8px;"></div>
        </div>
        <div class="demo-segment" data-segment="4" style="flex: 0 0 280px; background: #f9fafb; padding: 1.5rem; border-radius: 8px; cursor: pointer; border-bottom: 3px solid #e5e7eb; transition: all 0.3s ease; position: relative;">
          <h5 class="title is-6" style="margin-bottom: 0.5rem; color: #374151;">00:37-00:49</h5>
          <p style="font-size: 0.85rem; color: #6b7280; margin: 0;">Detailed Events</p>
          <div class="segment-indicator" style="position: absolute; bottom: 0; left: 0; right: 0; height: 3px; background: #95A5B0; border-radius: 0 0 8px 8px;"></div>
        </div>
        <div class="demo-segment" data-segment="5" style="flex: 0 0 280px; background: #f9fafb; padding: 1.5rem; border-radius: 8px; cursor: pointer; border-bottom: 3px solid #e5e7eb; transition: all 0.3s ease; position: relative;">
          <h5 class="title is-6" style="margin-bottom: 0.5rem; color: #374151;">00:50-00:59</h5>
          <p style="font-size: 0.85rem; color: #6b7280; margin: 0;">Detailed Events</p>
          <div class="segment-indicator" style="position: absolute; bottom: 0; left: 0; right: 0; height: 3px; background: #95A5B0; border-radius: 0 0 8px 8px;"></div>
        </div>
      </div>
    </div>

    <!-- Detailed Information Panel with Grid Layout -->
    <div id="detailPanel" style="background: white; border-radius: 12px; padding: 2.5rem; box-shadow: 0 4px 16px rgba(0,0,0,0.08); margin-bottom: 2rem; display: none;">
      <h4 id="detailTitle" class="title is-4" style="margin-bottom: 2rem; color: #111827;">Time Segment Details</h4>
      
      <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 2rem;">
        <div class="dimension-card" style="margin-bottom: 0;">
          <div style="display: flex; align-items: center; margin-bottom: 1rem;">
            <span style="font-size: 1.2rem; margin-right: 0.5rem;">üìù</span>
            <span class="tag" style="background: hsl(204, 86%, 53%); color: white; font-weight: 600; font-size: 0.9rem;">Detailed Events</span>
          </div>
          <p id="dim-segment_detail_caption" style="line-height: 1.7; color: #4b5563; margin: 0;">-</p>
        </div>

        <div class="dimension-card" style="margin-bottom: 0;">
          <div style="display: flex; align-items: center; margin-bottom: 1rem;">
            <span style="font-size: 1.2rem; margin-right: 0.5rem;">üèûÔ∏è</span>
            <span class="tag" style="background: hsl(204, 86%, 53%); color: white; font-weight: 600; font-size: 0.9rem;">Visual Background</span>
          </div>
          <p id="dim-video_background" style="line-height: 1.7; color: #4b5563; margin: 0;">-</p>
        </div>

        <div class="dimension-card" style="margin-bottom: 0;">
          <div style="display: flex; align-items: center; margin-bottom: 1rem;">
            <span style="font-size: 1.2rem; margin-right: 0.5rem;">üé•</span>
            <span class="tag" style="background: hsl(204, 86%, 53%); color: white; font-weight: 600; font-size: 0.9rem;">Camera State</span>
          </div>
          <p id="dim-camera_state" style="line-height: 1.7; color: #4b5563; margin: 0;">-</p>
        </div>

        <div class="dimension-card" style="margin-bottom: 0;">
          <div style="display: flex; align-items: center; margin-bottom: 1rem;">
            <span style="font-size: 1.2rem; margin-right: 0.5rem;">üé¨</span>
            <span class="tag" style="background: hsl(204, 86%, 53%); color: white; font-weight: 600; font-size: 0.9rem;">Shot Editing Style</span>
          </div>
          <p id="dim-shooting_style" style="line-height: 1.7; color: #4b5563; margin: 0;">-</p>
        </div>

        <div class="dimension-card" style="margin-bottom: 0;">
          <div style="display: flex; align-items: center; margin-bottom: 1rem;">
            <span style="font-size: 1.2rem; margin-right: 0.5rem;">üí¨</span>
            <span class="tag" style="background: hsl(204, 86%, 53%); color: white; font-weight: 600; font-size: 0.9rem;">Dialogue Content</span>
          </div>
          <p id="dim-speech_content" style="line-height: 1.7; color: #4b5563; margin: 0;">-</p>
        </div>

        <div class="dimension-card" style="margin-bottom: 0;">
          <div style="display: flex; align-items: center; margin-bottom: 1rem;">
            <span style="font-size: 1.2rem; margin-right: 0.5rem;">üîä</span>
            <span class="tag" style="background: hsl(204, 86%, 53%); color: white; font-weight: 600; font-size: 0.9rem;">Acoustics Content</span>
          </div>
          <p id="dim-acoustics_content" style="line-height: 1.7; color: #4b5563; margin: 0;">-</p>
        </div>
      </div>
    </div>
  </div>
</section>

<script>
  const demoData = [
    {
      timestamp: "00:00-00:10",
      segment_detail_caption: "A woman with long dark hair, wearing a black blazer over a white collared shirt, has a deeply sorrowful and worried expression. Her eyes are red-rimmed and welling with tears as she listens to a man's voiceover. The scene then cuts to a wide shot of her walking a white Labrador guide dog on a leash. The dog wears a yellow guide dog vest. They walk along a sidewalk next to a black metal fence and a brick building.",
      camera_state: "The scene starts with a close-up on the woman's face, focusing on her emotional state. It then cuts to a wide, long shot, tracking her and the dog as they walk from right to left.",
      video_background: "The initial background is a blurry, overcast outdoor setting with a building in the distance. The second part of the scene is set on a sidewalk next to a modern building with a brick base and a tall metal fence. The weather appears overcast and cool.",
      shooting_style: "The scene uses a standard cut to transition from a close-up to a wide shot, establishing the character and her environment.",
      speech_content: "Man (voiceover): ...but because of your serious violation of regulations. You abandoned your post during your internship, and to make things worse, you acted on impulses, resulting in serious consequences.",
      acoustics_content: "1) Tone of speech: The man's voice is serious, formal, and accusatory. 2) Background sounds or music: A low, somber, and slightly tense musical score plays in the background."
    },
    {
      timestamp: "00:11-00:20",
      segment_detail_caption: "The woman, Xiaoxin, continues walking with a determined but somber expression. Her hair blows gently in the wind. The man's voiceover continues, expressing sympathy for her loss but emphasizing that her mistake was irreversible. The scene then shows her walking across a crosswalk as the pedestrian light is red.",
      camera_state: "The camera alternates between a medium shot of Xiaoxin walking towards it and a wide shot of her crossing a street. The camera is mostly static.",
      video_background: "The background consists of bare trees, a sidewalk, and a distant city skyline under a hazy, overcast sky. The setting feels desolate and cold.",
      shooting_style: "The scene uses intercutting between different angles of the character walking to convey her journey and emotional state.",
      speech_content: "Man (voiceover): I'm sympathetic that your brother died, and it was devastating for you. But this mistake is one that can never be remedied. If you ask me, I think you better have a second thought.",
      acoustics_content: "1) Tone of speech: The man's tone remains serious and firm. 2) Background sounds or music: The somber, tense music continues."
    },
    {
      timestamp: "00:21-00:28",
      segment_detail_caption: "Xiaoxin stops at a crosswalk and presses the button on a white pedestrian crossing alert device. The device has a red button and Chinese characters on a sticker. The camera then reveals a critical detail: the thick white cable connecting the device to the pole is cut, dangling loosely. The red pedestrian light is still illuminated.",
      camera_state: "The camera starts with a close-up on Xiaoxin's hand as she presses the button. It then tilts up to reveal the cut cable in a close-up shot, creating a moment of suspense.",
      video_background: "The scene is at a crosswalk with a metal pole and a white pedestrian crossing signal box. The background is a paved sidewalk and a grey, tiled surface.",
      shooting_style: "A close-up reveal is used to highlight the crucial detail of the cut cable, shifting the tone from emotional to suspenseful.",
      speech_content: "Narrator: Xiaoxin stood at the intersection and pressed the button on the blind pedestrian crossing alert. But what she didn't expect was that the wire of this electrical box had long been cut.",
      acoustics_content: "1) Tone of speech: The narrator's voice is calm and descriptive. 2) Background sounds or music: A low, ominous sound effect plays when the cut cable is revealed."
    },
    {
      timestamp: "00:29-00:36",
      segment_detail_caption: "A man in a dark jacket runs up to the crosswalk, glances at the red light, and immediately starts crossing the street without hesitation. Xiaoxin, who is still waiting, hears his footsteps and turns her head to watch him. The man runs across the entire crosswalk, ignoring the red light.",
      camera_state: "A wide shot shows the man running across the crosswalk. The camera then cuts to a close-up of Xiaoxin's face as she reacts to the sound of his footsteps, followed by a shot of the man from behind as he runs.",
      video_background: "The scene is a crosswalk on a city street with a pedestrian traffic light, bare trees, and modern buildings in the background. The sky is overcast.",
      shooting_style: "The scene uses a combination of wide shots and reaction shots to build suspense and show the spatial relationship between the two characters.",
      speech_content: "Narrator: At this moment, a man ran to the roadside, looked around, and then rushed across. He didn't care about the red light on the zebra crossing.",
      acoustics_content: "1) Tone of speech: The narrator's voice is descriptive. 2) Background sounds or music: The sound of footsteps on pavement is prominent, and the tense musical score begins to build."
    },
    {
      timestamp: "00:37-00:49",
      segment_detail_caption: "Xiaoxin, who is blind, begins to walk across the crosswalk, relying on her guide dog. The scene transitions to a dark, abstract representation of her world, where she and the dog are glowing white figures. The red light is a faint, distant green light. Suddenly, a car screeches to a halt just in front of them, its headlights cutting through the darkness.",
      camera_state: "The camera follows Xiaoxin from behind as she walks. It then transitions to a dark, abstract space with a point-of-view shot from the car's perspective, looking at the approaching figures. The camera is shaky to simulate the car's sudden stop.",
      video_background: "The scene transitions from a real-world crosswalk to a black, abstract void, symbolizing Xiaoxin's blindness. The only elements are the glowing figures and the distant light.",
      shooting_style: "A special effect sequence is used, transitioning from reality to a symbolic representation of the character's perception to heighten the sense of peril.",
      speech_content: "Narrator: Xiaoxin thought the green light had already turned on, so she hurried and cautiously stepped onto the zebra crossing. Xiaoxin's world was pitch black. Only the green light was a faint glow in the distance. Suddenly, a car screeched to a halt in front of Xiaoxin.",
      acoustics_content: "1) Tone of speech: The narrator's voice is dramatic. 2) Background sounds or music: The sound of screeching tires is loud and jarring, followed by a tense, high-pitched musical sting."
    },
    {
      timestamp: "00:50-00:59",
      segment_detail_caption: "The car's driver, a man with a buzz cut, angrily yells at Xiaoxin, asking if she's trying to kill herself. Xiaoxin freezes, her face a mask of shock and fear. The red pedestrian light is now clearly visible, flashing in the background. She looks around, disoriented and terrified, as the car's horn blares.",
      camera_state: "The camera cuts between a close-up of the angry driver, a close-up of Xiaoxin's terrified face, and a shot of the flashing red light. The camera is handheld and shaky, reflecting the chaotic and frightening moment.",
      video_background: "The scene is back in the real world, on the crosswalk. The flashing red light is the most prominent background element, indicating danger.",
      shooting_style: "Rapid cuts and shaky, handheld camera work are used to convey the chaos and fear of the moment.",
      speech_content: "Driver: Tired of living?! Open your eyes when crossing the road! Almost got yourself killed!",
      acoustics_content: "1) Tone of speech: The driver's voice is loud, angry, and aggressive. 2) Background sounds or music: The sound of a car horn blaring repeatedly and a tense, dramatic score underscore the scene."
    }
  ];

  // Add click handlers to segment blocks
  document.querySelectorAll('.demo-segment').forEach((segment, index) => {
    segment.addEventListener('click', function() {
      const data = demoData[index];
      
      // Update detail panel
      document.getElementById('detailTitle').textContent = 'Time Segment: ' + data.timestamp;
      document.getElementById('dim-segment_detail_caption').textContent = data.segment_detail_caption;
      document.getElementById('dim-video_background').textContent = data.video_background;
      document.getElementById('dim-camera_state').textContent = data.camera_state;
      document.getElementById('dim-shooting_style').textContent = data.shooting_style;
      document.getElementById('dim-speech_content').textContent = data.speech_content;
      document.getElementById('dim-acoustics_content').textContent = data.acoustics_content;
      
      // Show detail panel
      document.getElementById('detailPanel').style.display = 'block';
      
      // Update video time
      const times = data.timestamp.split('-');
      const startTime = timeToSeconds(times[0]);
      const video = document.getElementById('demoVideo');
      if (video) {
        video.currentTime = startTime;
      }
      
      // Highlight selected segment
      document.querySelectorAll('.demo-segment').forEach(s => s.classList.remove('active'));
      this.classList.add('active');
    });
  });
  
  function timeToSeconds(timeStr) {
    const parts = timeStr.split(':');
    return parseInt(parts[0]) * 60 + parseInt(parts[1]);
  }
</script>

<section class="hero teaser">
  <div class="container" style="padding: 20px; max-width: 1000px; margin: 0 auto;">
    <h2 class="title is-3 has-text-centered" style="margin-bottom: 1.5rem;">Abstract</h2>
    
    <div class="content has-text-justified" style="margin-bottom: 2rem; max-width: 1000px; margin-left: auto; margin-right: auto;">
      <p>
We introduce <strong>TimeChat-Captioner</strong>, a model that generates script-like, temporally dense, and structurally rich audio-visual narratives. Unlike traditional video captioning approaches that produce single-sentence summaries, TimeChat-Captioner provides temporally-dense scene segmentation with explicit MM:SS timestamps and description-dense structured captions spanning six dimensions: <strong>Detailed Events</strong> (narrating audiovisual content and actions), <strong>Visual Background</strong> (depicting setting, location, and atmosphere), <strong>Camera State</strong> (describing camera movements, angles, and framing), <strong>Shot Editing Style</strong> (analyzing post-production editing), <strong>Dialogue Content</strong> (transcription and summary with speaker ID), and <strong>Acoustics Content</strong> (portraying background sounds, music, and speaking tones).
</p>

<p>
To address the challenge of evaluating continuous captions with subjective boundary ambiguity, we propose <strong>SodaM</strong>, a unified metric that employs a two-stage alignment strategy combining IoU-based Dynamic Programming with many-to-one merging, and measures semantic completeness through LLM-based CheckList scoring.
</p>

<p>
We construct two complementary datasets: <strong>OmniDCBench</strong> (1,122 videos with human annotations, averaging 995 words per video) as a high-quality benchmark, and <strong>TimeChatCap-42K</strong> (42,000 synthesized video-caption pairs) for training. Our model, <strong>TimeChat-Captioner-7B</strong>, built on Qwen2.5-Omni with multimodal RoPE for precise temporal localization, employs a two-stage training strategy: supervised fine-tuning followed by Group Relative Policy Optimization (GRPO) that jointly optimizes temporal accuracy and caption quality through a composite reward function.
</p>

<p>
<strong>TimeChat-Captioner-7B</strong> achieves a SodaM score of <strong>35.0</strong>, surpassing Gemini-2.5-Pro (33.7) and Gemini-2.5-Flash (30.0), while significantly outperforming previous open-source models. The model shows particular strength in Camera (12.4), Acoustics (38.2), and Dialogue (54.3) dimensions and generalizes effectively to downstream tasks, achieving 52.8 on DailyOmni, 22.6 on WorldSense, and 48.3 R1@0.7 on Charades-STA, outperforming specialized expert models.
      </p>
      
    </div>
  </div>
</section>

<!-- Time-Aware and Structural Audio-Visual Captions task ÂÆö‰πâ -->
<section class="section" style="margin-top: 1rem; margin-bottom: 2rem;">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Time-Aware and Structural Audio-Visual Captions</h2>
    
    <div class="content has-text-justified" style="margin-bottom: 2rem; max-width: 1000px; margin-left: auto; margin-right: auto;">
    <p style="font-size: 1.1rem;">
        <strong>TimeChat-Captioner</strong> generates script-like, temporally dense, and structurally rich audio-visual narratives for multi-scene videos. At its core is the innovative <strong>six-dimensional structural schema</strong> that comprehensively describes video content like a professional film script.
    </p>
    <p style="font-size: 1.05rem;">
      The <strong>six structural dimensions</strong> include:
    </p>
    <ul style="font-size: 1.05rem; list-style-type: disc; margin-left: 2rem;">
      <li><strong>Detailed Events:</strong> narrating audiovisual content and actions</li>
      <li><strong>Visual Background:</strong> depicting setting, location, and atmosphere</li>
      <li><strong>Camera State:</strong> describing camera movements, angles, and framing</li>
      <li><strong>Shot Editing Style:</strong> analyzing post-production editing</li>
      <li><strong>Dialogue Content:</strong> transcription and summary with speaker ID</li>
      <li><strong>Acoustics Content:</strong> portraying background sounds, music, and speaking tones</li>
    </ul>
    </div>
    <div style="margin-bottom: 2.5rem;">
      <img src="static/images/teaser.png" alt="TimeChat-Captioner Task Overview" style="width: 100%; border-radius: 4px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);">
      <p style="font-size: 0.9rem; color: #95A5B0; text-align: center; margin-top: 0.5rem;">
        TimeChat-Captioner generates temporally-dense and description-dense captions with six structural dimensions.
      </p>
    </div>
  </div>
</section>

<!-- New section about SodaM Metric and Model Architecture -->
<section class="section" id="method" style="margin-top: 1rem; margin-bottom: 2rem;">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">SodaM: A Unified Evaluation Metric</h2>
    
    <div class="content has-text-justified" style="margin-bottom: 2rem; max-width: 1000px; margin-left: auto; margin-right: auto;">
      <p>
        Evaluating continuous captions faces the <strong>"boundary ambiguity"</strong> challenge‚Äîscene boundaries are subjective, and predictions may be more fine-grained than ground truth. We propose <strong>SodaM</strong>, a unified metric that addresses these challenges through a sophisticated two-stage alignment strategy.
      </p>
    </div>
    <div class="content has-text-justified" style="margin-bottom: 2rem; max-width: 1000px; margin-left: auto; margin-right: auto;">
        <h4 class="title is-5">How SodaM Works</h4>

        <ol style="list-style-type: decimal; margin-left: 2rem;">
          <li>
            <strong>IoU-based Dynamic Programming Alignment:</strong> Uses Dynamic Programming to find the optimal alignment path through the (M, N) scene grid based on temporal Intersection over Union (IoU), ensuring the best match between predicted and ground-truth scene boundaries.
          </li>
          <li>
            <strong>Many-to-One Merging:</strong> Gracefully handles cases where models generate finer-grained segments than human references by concatenating predicted captions for a single ground-truth match, avoiding unfair penalties for detailed predictions.
          </li>
          <li>
            <strong>CheckList Score for Semantic Completeness:</strong> Uses an LLM (Gemini-2.5-Flash) to judge if predicted captions cover atomic elements (keypoints) from the ground truth, providing a holistic measure of semantic coverage across all six dimensions.
          </li>
        </ol>
    </div>

    <h2 class="title is-3 has-text-centered">TimeChat-Captioner-7B Architecture</h2>
    
    <div class="content has-text-justified" style="margin-bottom: 2rem; max-width: 1000px; margin-left: auto; margin-right: auto;">
      <p>
        <strong>TimeChat-Captioner-7B</strong> is built on Qwen2.5-Omni with key architectural innovations for precise temporal localization and synchronous audio-visual perception.
      </p>
    </div>
    
    <div class="content has-text-justified" style="margin-bottom: 2rem; max-width: 1000px; margin-left: auto; margin-right: auto;">
      <h4 class="title is-5">Key Architectural Features</h4>
      <ul class="feature-list" style="list-style-type: none; margin-left: 0;">
        <li>
          <strong>Synchronous Perception:</strong>
          <p>Interleaved audio and visual tokens in a single sequence, enabling holistic audio-visual understanding without separate processing streams.</p>
        </li>
        <li>
          <strong>Multimodal RoPE (M-RoPE):</strong>
          <p>Multimodal Rotary Position Embedding that encodes precise absolute temporal information, crucial for generating accurate MM:SS timestamps in captions.</p>
        </li>
        <li>
          <strong>Two-Stage Training (SFT + GRPO):</strong>
          <p>Stage 1: Supervised Fine-Tuning on the TimeChatCap-42K dataset teaches the complex "script" format. Stage 2: Group Relative Policy Optimization (GRPO) jointly optimizes temporal accuracy and caption quality through a composite reward function combining multiple reward signals.</p>
        </li>
      </ul>
    </div>

    <!-- Reward Function Details -->
    <div class="content has-text-justified" style="margin-bottom: 2rem; max-width: 1000px; margin-left: auto; margin-right: auto;">
      <h4 class="title is-5">Composite Reward Function</h4>
      <ul style="list-style-type: disc; margin-left: 2rem;">
        <li><strong>Format Reward:</strong> Binary reward for valid JSON-formatted output (1 if parseable, 0 otherwise).</li>
        <li><strong>Length Reward:</strong> Regularizes output length to prevent hallucinations and repetitive content.</li>
        <li><strong>Timestamp Reward:</strong> Average F1 score at IoU thresholds {0.3, 0.5, 0.7, 0.9} for temporal accuracy.</li>
        <li><strong>Time-aware Caption Reward:</strong> Uses the SodaM metric to measure semantic completeness and temporal alignment across all six dimensions.</li>
      </ul>
    </div>

    <!-- Model Architecture figure -->
    <div style="margin-bottom: 2.5rem; max-width: 1000px; margin-left: auto; margin-right: auto;">
      <img src="static/images/model_architecture.png" alt="TimeChat-Captioner-7B Architecture" style="width: 100%; border-radius: 4px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);">
      <p style="font-size: 0.9rem; color: #95A5B0; text-align: center; margin-top: 0.5rem;">
        The architecture of TimeChat-Captioner-7B with synchronous audio-visual perception and M-RoPE for temporal localization.
      </p>
    </div>
  </div>
</section>
<!-- Dataset Information Section -->
<section class="section" id="dataset" style="margin-top: 0; padding-top: 0.5rem; margin-bottom: 1rem;">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Dataset Construction</h2>
    
    <div class="content has-text-justified" style="margin-bottom: 2rem; max-width: 1000px; margin-left: auto; margin-right: auto;">
      <p>
        We construct two complementary datasets to support the dense video captioning task: <strong>OmniDCBench</strong> as a high-quality human-annotated benchmark and <strong>TimeChatCap-42K</strong> as a large-scale training set.
      </p>
    </div>

    <div class="content has-text-justified" style="margin-bottom: 2rem; max-width: 1000px; margin-left: auto; margin-right: auto;">
      <h4 class="title is-5">OmniDCBench: Human Benchmark</h4>
      <p>
        <strong>OmniDCBench</strong> is entirely human-annotated by experts with cinematography knowledge, ensuring high-quality annotations that capture the nuances of professional video production.
      </p>
    </div>
    
    <!-- Card-style dataset statistics for OmniDCBench -->
    <div class="box" style="margin-bottom: 2.5rem; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1); border-radius: 8px; background-color: #f9f9fd; border-top: 5px solid #95A5B0; max-width: 1000px; margin-left: auto; margin-right: auto;">
        <h4 class="title is-5" style="margin-bottom: 1rem;">OmniDCBench Statistics:</h4>
        <ul style="list-style-type: disc; margin-left: 2rem;">
          <li style="margin-bottom: 1rem;">
            <strong>Scale:</strong> 1,122 high-resolution video clips
          </li>
          <li style="margin-bottom: 1rem;">
            <strong>Annotation Depth:</strong> Averaging 995 words per video
          </li>
          <li style="margin-bottom: 1rem;">
            <strong>Expert Annotation:</strong> Entirely human-annotated by experts with cinematography knowledge
          </li>
          <li style="margin-bottom: 1rem;">
            <strong>Coverage:</strong> All six structural dimensions (Detailed Events, Visual Background, Camera State, Shot Editing Style, Dialogue Content, Acoustics Content)
          </li>
        </ul>
        <img src="static/images/OnimiDCBench_stat.png" alt="Statistics of human-annotated OmniDCBench" style="width: 100%; border-radius: 4px; margin-top: 1rem;">
      </div>

    <div class="content has-text-justified" style="margin-bottom: 2rem; max-width: 1000px; margin-left: auto; margin-right: auto;">
      <h4 class="title is-5">TimeChatCap-42K: Training Set</h4>
      <p>
        <strong>TimeChatCap-42K</strong> is synthesized using Gemini-2.5-Pro via a three-stage pipeline: (1) Boundary Segmentation for establishing rough timestamps, (2) Detailed Caption Generation for expanding into the six-dimensional structural schema, and (3) Quality Filtering to ensure high-quality annotations.
      </p>
    </div>

    <!-- Card-style dataset statistics for TimeChatCap-42K -->
    <div class="box" style="margin-bottom: 2.5rem; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1); border-radius: 8px; background-color: #f9f9fd; border-top: 5px solid #95A5B0; max-width: 1000px; margin-left: auto; margin-right: auto;">
        <h4 class="title is-5" style="margin-bottom: 1rem;">TimeChatCap-42K Statistics:</h4>
        <ul style="list-style-type: disc; margin-left: 2rem;">
          <li style="margin-bottom: 1rem;">
            <strong>Scale:</strong> 42,000 high-quality video-script pairs
          </li>
          <li style="margin-bottom: 1rem;">
            <strong>Synthesis Pipeline:</strong> Three-stage process using Gemini-2.5-Pro (Boundary Segmentation ‚Üí Detailed Caption Generation ‚Üí Quality Filtering)
          </li>
          <li style="margin-bottom: 1rem;">
            <strong>Quality:</strong> Structured captions following the six-dimensional schema
          </li>
          <li style="margin-bottom: 1rem;">
            <strong>Purpose:</strong> Large-scale training for teaching the model complex "script" format
          </li>
        </ul>
        <img src="static/images/train_stat.png" alt="Statistics of the training dataset TimeChatCap-42K" style="width: 100%; border-radius: 4px; margin-top: 1rem;">
      </div>
    </div>
    
    <!-- Dataset Example Visualization -->
    <div class="container is-max-desktop">
      <div class="content has-text-justified" style="margin-bottom: 2rem; max-width: 1000px; margin-left: auto; margin-right: auto;">
        <img src="static/images/TimeChatCap42k_pipeline.png" alt="Overview of the synthetic training data construction pipeline for the training dataset TimeChatCap-42K" style="width: 100%; border-radius: 4px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);">
        <p style="font-size: 0.9rem; color: #95A5B0; text-align: center; margin-top: 0.5rem;">
          Overview of the synthetic training data construction pipeline for the training dataset TimeChatCap-42K.
        </p>
      </div>
    </div>
  </div>
</section>

<!-- End Dataset Section -->

<!-- Experiments Section -->
<section class="section" id="experiments" style="margin-top: 1rem; margin-bottom: 2rem;">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Experimental Results</h2>
    
    <!-- Table 1: Main Results on OmniDCBench -->
    <div class="content has-text-centered" style="margin-bottom: 2rem; max-width: 1000px; margin-left: auto; margin-right: auto;">
      <h4 class="title is-5 has-text-centered">Main Results on OmniDCBench</h4>
      
      <p>
        <strong>TimeChat-Captioner-7B</strong> achieves a SodaM score of <strong>35.0</strong>, surpassing Gemini-2.5-Pro (33.7) and Gemini-2.5-Flash (30.0), establishing state-of-the-art performance. The model significantly outperforms previous open-source models like Qwen3-Omni (14.3) and MiniCPM-o-2.6 (5.4).
      </p>
      
      <div style="overflow-x: auto; margin-bottom: 1rem;">
        <img src="static/images/main_results.png" alt="Main Results on OmniDCBench (Table 1)" style="width: 90%; border-radius: 4px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1); display: block; margin: 0 auto;">
      </div>
      
      <p style="font-size: 0.9rem; color: #95A5B0; text-align: center; margin-top: 0.5rem; margin-bottom: 2rem;">
        Main experimental results on OmniDCBench. TimeChat-Captioner-7B shows particular strength in Camera (12.4), Acoustics (38.2), and Dialogue (54.3) dimensions, which are traditionally difficult for MLLMs.
      </p>
    </div>
    
    <!-- Table 2: Downstream Generalization Results -->
    <div class="content has-text-centered" style="margin-bottom: 2rem; max-width: 1000px; margin-left: auto; margin-right: auto;">
      <h4 class="title is-5 has-text-centered">Downstream Generalization: Audio-Visual Reasoning</h4>
      
      <p>
        TimeChat-Captioner-7B demonstrates strong generalization to audio-visual reasoning tasks, achieving <strong>52.8</strong> on DailyOmni and <strong>22.6</strong> on WorldSense (best among open-source models), showing that the fine-grained audio-visual understanding learned from dense video captioning transfers effectively to other multimodal tasks.
      </p>
      
      <div style="overflow-x: auto; margin-bottom: 1rem;">
        <img src="static/images/table2_downstream.png" alt="Downstream Generalization Results (Table 2)" style="width: 80%; border-radius: 4px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1); display: block; margin: 0 auto;">
      </div>
      
      <p style="font-size: 0.9rem; color: #95A5B0; text-align: center; margin-top: 0.5rem; margin-bottom: 2rem;">
        Performance on audio-visual reasoning benchmarks (DailyOmni and WorldSense).
      </p>
    </div>
    
    <!-- Table 3: Temporal Grounding Results -->
    <div class="content has-text-centered" style="margin-bottom: 2rem; max-width: 1000px; margin-left: auto; margin-right: auto;">
      <h4 class="title is-5 has-text-centered">Temporal Grounding on Charades-STA</h4>
      
      <p>
        On the temporal grounding task Charades-STA, TimeChat-Captioner-7B achieves an R1@0.7 of <strong>48.3</strong>, outperforming specialized expert models like TimeSuite (43.0). This demonstrates that the precise temporal localization capabilities learned through M-RoPE and GRPO training generalize effectively to temporal grounding tasks.
      </p>
      
      <div style="overflow-x: auto; margin-bottom: 1rem;">
        <img src="static/images/table3_temporal.png" alt="Temporal Grounding Results (Table 3)" style="width: 80%; border-radius: 4px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1); display: block; margin: 0 auto;">
      </div>
      
      <p style="font-size: 0.9rem; color: #95A5B0; text-align: center; margin-top: 0.5rem; margin-bottom: 1rem;">
        Results on Charades-STA temporal grounding benchmark, showing superior performance compared to specialized models.
      </p>
      
      
    </div>
  </div>
</section>
<!-- End Experiments Section -->



<!--BibTeX citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{yao2026timechat,
  title={TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions},
  author={Yao, Linli and Wei, Yuancheng and Zhang, Yaojie and Li, Lei and Chen, Xinlong and Song, Feifan and Wang, Ziyue and Ouyang, Kun and Liu, Yuanxin and Kong, Lingpeng and Liu, Qi and Wan, Pengfei and Gai, Kun and Zhang, Yuanxing and Sun, Xu},
  journal={arXiv preprint arXiv:2602.08711},
  year={2026}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->

<!-- Acknowledgement section -->
<section class="section" id="Acknowledgement">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgement</h2>
    <p>
      <strong>Usage and License Notices:</strong> The data, code and checkpoints are intended and licensed for research use only. They are also restricted to uses that follow the license agreements of the respective datasets and models used in this work.
    </p>
    <p>
      <strong>Related Projects:</strong> 
      <a href="https://github.com/QwenLM/Qwen2.5-Omni" target="_blank">Qwen2.5-Omni</a>, 
      <a href="https://github.com/google/gemini" target="_blank">Gemini</a>
    </p>
  </div>
</section>
<!-- End Acknowledgement section -->

  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
</html>
